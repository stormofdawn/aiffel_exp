{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11dae892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf                            ##os는 자꾸 왜 불러오는거지, re는 또 뭐구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb456411",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()             ##자동으로 행별 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83997fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ab058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.    -> 대화가 아니라 화자가 누군지 표기한거라서\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6b3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문장들을 토큰화 해주는 방법\n",
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce35460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446e55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []           # 문장 들어갈 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d315c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue    # 아무것도 없는 공백\n",
    "    if sentence[-1] == \":\": continue   # :로 끝나는 화자표시\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9b4aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c9db941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f8590fb0370>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):      #토큰화 함수\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\")  # 7000개를 토큰으로, 넘치는건 unk로\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  # 토큰화된텍스트를 텐서라는 변수에 넣고\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  #텐서로 변환해주는 함수인가봄\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb792b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a3c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf67d557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9276f",
   "metadata": {},
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a973436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.num_words + 1   #패딩문자 : 할당된 비트수를 맞춰주기위해 0으로 채운값, 여기서 1에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25921e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))      # 텐서를 데이터셋 속에 넣음.\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed28e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05db29ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[ 2.16087166e-04, -2.87092233e-04,  1.13450806e-04, ...,\n",
       "          1.20215060e-04, -8.62177549e-05, -1.09402055e-04],\n",
       "        [ 1.52047432e-04, -7.69345323e-04,  1.76481481e-04, ...,\n",
       "          2.95109756e-04, -6.36725890e-05, -3.81494232e-04],\n",
       "        [ 1.88440536e-04, -1.35809788e-03,  3.48108617e-04, ...,\n",
       "          5.20972710e-04, -2.26982040e-04, -4.38983698e-04],\n",
       "        ...,\n",
       "        [ 6.18554041e-05,  1.07557094e-03,  2.35607941e-03, ...,\n",
       "         -7.11598143e-04, -4.81768249e-04, -2.20777723e-03],\n",
       "        [-5.20847752e-05,  1.24820881e-03,  2.42381147e-03, ...,\n",
       "         -5.59397333e-04, -3.62589839e-04, -2.36866227e-03],\n",
       "        [-1.26954372e-04,  1.41539646e-03,  2.50698673e-03, ...,\n",
       "         -4.10016655e-04, -2.58310378e-04, -2.51017231e-03]],\n",
       "\n",
       "       [[ 2.16087166e-04, -2.87092233e-04,  1.13450806e-04, ...,\n",
       "          1.20215060e-04, -8.62177549e-05, -1.09402055e-04],\n",
       "        [ 4.94466920e-04, -4.40303149e-04,  2.75285332e-04, ...,\n",
       "          9.74867653e-05, -4.32452623e-04, -7.53410714e-05],\n",
       "        [ 5.83325513e-04, -3.01833643e-04,  6.49472116e-04, ...,\n",
       "          2.47600547e-04, -5.88628231e-04, -2.79843534e-05],\n",
       "        ...,\n",
       "        [-8.95620789e-04,  1.55928207e-03,  2.31541111e-03, ...,\n",
       "         -2.13354782e-04,  6.52707880e-04, -1.94176042e-03],\n",
       "        [-8.29032157e-04,  1.71265972e-03,  2.49387464e-03, ...,\n",
       "         -1.49781685e-04,  6.43003383e-04, -2.12551397e-03],\n",
       "        [-7.35407113e-04,  1.84984168e-03,  2.65954947e-03, ...,\n",
       "         -8.20448258e-05,  6.23130938e-04, -2.28122855e-03]],\n",
       "\n",
       "       [[ 2.16087166e-04, -2.87092233e-04,  1.13450806e-04, ...,\n",
       "          1.20215060e-04, -8.62177549e-05, -1.09402055e-04],\n",
       "        [ 4.27726656e-04, -6.45246531e-04,  1.07026804e-04, ...,\n",
       "         -8.17889595e-05, -2.39487941e-04, -4.04793100e-04],\n",
       "        [ 4.51883243e-04, -4.40402509e-04, -1.00567195e-04, ...,\n",
       "         -9.76920855e-05, -7.31091306e-04, -5.92537457e-04],\n",
       "        ...,\n",
       "        [-8.01490241e-05,  2.30330299e-03,  2.72914348e-03, ...,\n",
       "          6.59384299e-04,  2.00580471e-04, -2.48543429e-03],\n",
       "        [-2.69420489e-05,  2.35481630e-03,  2.86191748e-03, ...,\n",
       "          6.78348879e-04,  2.43523536e-04, -2.57951999e-03],\n",
       "        [ 3.83590632e-05,  2.38973671e-03,  2.97372998e-03, ...,\n",
       "          6.98478252e-04,  2.75697064e-04, -2.65623303e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.16087166e-04, -2.87092233e-04,  1.13450806e-04, ...,\n",
       "          1.20215060e-04, -8.62177549e-05, -1.09402055e-04],\n",
       "        [ 2.33287050e-04, -4.75809997e-04,  3.78370169e-04, ...,\n",
       "          1.46773295e-04, -2.48849770e-04, -2.31267535e-04],\n",
       "        [ 9.57579759e-05, -6.06104964e-04,  2.35947184e-04, ...,\n",
       "          7.97488683e-05, -5.40121226e-04, -2.12899307e-04],\n",
       "        ...,\n",
       "        [-3.12743388e-04,  1.75025116e-03,  1.84604444e-03, ...,\n",
       "         -2.99291714e-04,  6.29147544e-05, -1.50450750e-03],\n",
       "        [-3.22568376e-04,  1.90142158e-03,  2.03752238e-03, ...,\n",
       "         -2.39055589e-04,  1.94483277e-04, -1.78155408e-03],\n",
       "        [-3.00610205e-04,  2.03280966e-03,  2.22853106e-03, ...,\n",
       "         -1.65106321e-04,  2.95264967e-04, -2.01164605e-03]],\n",
       "\n",
       "       [[ 2.16087166e-04, -2.87092233e-04,  1.13450806e-04, ...,\n",
       "          1.20215060e-04, -8.62177549e-05, -1.09402055e-04],\n",
       "        [ 3.86804168e-05, -3.79381207e-04, -5.27225202e-05, ...,\n",
       "          7.27440784e-05, -4.45042300e-04, -4.63548931e-04],\n",
       "        [ 6.78794386e-05, -4.55905887e-04, -2.98065686e-04, ...,\n",
       "         -2.63121503e-04, -5.78606036e-04, -5.04042720e-04],\n",
       "        ...,\n",
       "        [-1.43693076e-04,  1.66373805e-03,  2.82880734e-03, ...,\n",
       "          1.07237691e-04,  9.24261345e-04, -2.01465911e-03],\n",
       "        [-1.53868721e-04,  1.82050711e-03,  2.89515615e-03, ...,\n",
       "          1.77017166e-04,  8.38681823e-04, -2.15383968e-03],\n",
       "        [-1.37262439e-04,  1.95349962e-03,  2.96522281e-03, ...,\n",
       "          2.41942471e-04,  7.60546187e-04, -2.27721152e-03]],\n",
       "\n",
       "       [[ 2.16087166e-04, -2.87092233e-04,  1.13450806e-04, ...,\n",
       "          1.20215060e-04, -8.62177549e-05, -1.09402055e-04],\n",
       "        [ 3.86804168e-05, -3.79381207e-04, -5.27225202e-05, ...,\n",
       "          7.27440784e-05, -4.45042300e-04, -4.63548931e-04],\n",
       "        [ 2.21678580e-04, -3.55029566e-04,  1.68202459e-04, ...,\n",
       "          5.35067957e-05, -5.40303125e-04, -6.13620388e-04],\n",
       "        ...,\n",
       "        [-4.03318234e-04,  1.17151800e-03,  2.49339221e-03, ...,\n",
       "         -2.05642908e-04,  6.78573415e-05, -1.70327630e-03],\n",
       "        [-4.56168869e-04,  1.39822508e-03,  2.62552849e-03, ...,\n",
       "         -1.46890481e-04,  1.80603776e-04, -1.92805356e-03],\n",
       "        [-4.65454097e-04,  1.59435638e-03,  2.75319046e-03, ...,\n",
       "         -7.68472673e-05,  2.65326205e-04, -2.11888435e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b46592f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c05e114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 39s 380ms/step - loss: 3.5467\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 37s 396ms/step - loss: 2.8202\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 38s 406ms/step - loss: 2.7468\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 37s 400ms/step - loss: 2.6656\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 2.5952\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 2.5391\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 38s 405ms/step - loss: 2.4848\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 34s 365ms/step - loss: 2.4355\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 2.3923\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 2.3537\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 38s 403ms/step - loss: 2.3171\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 38s 403ms/step - loss: 2.2826\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 38s 403ms/step - loss: 2.2500\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 2.2164\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 2.1862\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 2.1562\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 2.1253\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 38s 404ms/step - loss: 2.0960\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 34s 364ms/step - loss: 2.0644\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 38s 405ms/step - loss: 2.0352\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 38s 405ms/step - loss: 2.0037\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 38s 405ms/step - loss: 1.9726\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 1.9419\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.9115\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 1.8792\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 38s 403ms/step - loss: 1.8493\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 38s 404ms/step - loss: 1.8179\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 38s 404ms/step - loss: 1.7891\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.7588\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 38s 404ms/step - loss: 1.7306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f858f05f7c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c6f6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b2a4f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s not a man , and i am able to see <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "457fbabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hungry <unk> <unk> in the <unk> s death , <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> hungry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6d96222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> dog , sir , i will not . <end> '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f442f26",
   "metadata": {},
   "source": [
    "## 여기부터 프로젝트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bf73b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 가져오기부터...\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2639f48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/aiffel/aiffel/lyricist/data/lyrics/leonard-cohen.txt',\n",
       " '/aiffel/aiffel/lyricist/data/lyrics/lil-wayne.txt',\n",
       " '/aiffel/aiffel/lyricist/data/lyrics/blink-182.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db12fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a02379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1dd071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264885d",
   "metadata": {},
   "source": [
    "### 데이터 정제(전처리)\n",
    "\n",
    "1. 문장중 길이가 0이거나 15이상인 문장 제외\n",
    "2. 문장을 정규표현식을 사용해 기호, 띄어쓰기등이 없도록 변환\n",
    "\n",
    "### 토큰화\n",
    "\n",
    "1. 문장을 단어로 변환 후 각 단어에 인덱스를 붙여서 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "896658af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   \n",
    "    if len(sentence) > 75: continue\n",
    "    if idx > 5: break   \n",
    "        \n",
    "    print(sentence)     ## 15개는 너무 작은가보다. 애시당초 센텐스면 단어 개수가 아니라 알파벳으로 자르는건가\n",
    "                        ## 1줄에 대충 띄어쓰기 포함 5단어라고 치고 걸러보자  -> 나중에 행렬에서 열이 너무 많아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f0d8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()                       # 소문자, 좌우 공백삭제\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)       # 특수문자 좌우에 공백넣기\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)               # 2개이상의 공백은 1개로\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)     #  특수문자 = 공백 치환\n",
    "    sentence = sentence.strip()                               # 공백 삭제\n",
    "    sentence = '<start> ' + sentence + ' <end>'               #문장 처음과 끝에 <start>, <end> 넣기\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0cbbf38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split()) > 15: continue          #오케이 됐다\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0fb0c1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> now i ve heard there was a secret chord <end>', '<start> that david played , and it pleased the lord <end>', '<start> but you don t really care for music , do you ? <end>', '<start> it goes like this <end>', '<start> the fourth , the fifth <end>']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:5])            #?가 보이는데 잘못된건가?, 아 ?, !는 빼고구나 잘 된거같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d81c81fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2971 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  117 ...    0    0    0]\n",
      " [   2  258  195 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f84961b3dc0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):     \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,    # 이전에 unk가 많이 나와서 좀 늘렸음\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\")  \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)               ## 토큰으로 잘 바뀌었다. 2로 시작하는거 보니 잘되었군"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75383b38",
   "metadata": {},
   "source": [
    "### 메모리 할당 / 데이터 셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37c0cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d4b6551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)) \n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0eaba3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##여기까지하고 분류하는게 맞겠지?\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ee9f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3da51555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a791f0",
   "metadata": {},
   "source": [
    "대충 성공적이다\n",
    "### 모델에 맞춰 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ffe4c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lyricist(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = lyricist(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4dc2e8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-1.32260684e-04,  1.76336165e-04,  2.23136449e-05, ...,\n",
       "          1.98939990e-04,  6.49734138e-05, -5.33336861e-05],\n",
       "        [-8.88010545e-05,  4.12185793e-04, -1.97884801e-04, ...,\n",
       "          1.09123110e-04,  2.05781507e-05, -2.32958730e-04],\n",
       "        [-9.07630019e-05,  5.77206258e-04, -4.07557236e-04, ...,\n",
       "          1.45193017e-05,  6.13477096e-05, -2.86252412e-04],\n",
       "        ...,\n",
       "        [ 5.88754134e-04, -1.97822286e-04,  2.10699000e-04, ...,\n",
       "         -5.38915803e-04,  2.47444957e-04, -4.06241015e-04],\n",
       "        [ 5.44317125e-04,  8.14986925e-05,  4.71767649e-04, ...,\n",
       "         -2.89131858e-04,  3.13071156e-04, -3.55269643e-04],\n",
       "        [ 4.09154600e-04,  4.59551520e-04,  5.55857201e-04, ...,\n",
       "         -3.73169933e-05,  4.86185862e-04, -3.34214972e-04]],\n",
       "\n",
       "       [[-1.32260684e-04,  1.76336165e-04,  2.23136449e-05, ...,\n",
       "          1.98939990e-04,  6.49734138e-05, -5.33336861e-05],\n",
       "        [-2.06493511e-04,  1.20087410e-04, -3.36565718e-05, ...,\n",
       "          3.69207439e-04,  7.52233391e-05, -5.72984281e-05],\n",
       "        [-2.68720585e-04,  5.70154516e-05, -5.30291931e-04, ...,\n",
       "          6.17208541e-04,  1.12495356e-04, -1.03485123e-04],\n",
       "        ...,\n",
       "        [-2.20118600e-04, -2.44658677e-05, -2.41748785e-04, ...,\n",
       "          2.63164635e-04,  1.00266666e-03, -1.74219289e-03],\n",
       "        [ 9.04175904e-05, -9.56013901e-05,  7.33832130e-05, ...,\n",
       "         -9.07440844e-05,  1.23053801e-03, -1.64289947e-03],\n",
       "        [ 4.30851185e-04, -3.46351560e-04,  4.77849768e-04, ...,\n",
       "         -5.01427858e-04,  1.27595488e-03, -1.59021874e-03]],\n",
       "\n",
       "       [[-1.32260684e-04,  1.76336165e-04,  2.23136449e-05, ...,\n",
       "          1.98939990e-04,  6.49734138e-05, -5.33336861e-05],\n",
       "        [-2.33247760e-04,  2.12676619e-04,  1.89265542e-04, ...,\n",
       "          3.18895414e-04, -2.82028159e-05, -2.70396442e-04],\n",
       "        [-3.77279794e-04,  1.57890769e-04,  4.53579676e-04, ...,\n",
       "          3.59247264e-04, -6.34743337e-05, -4.17601434e-04],\n",
       "        ...,\n",
       "        [-6.01862557e-04,  1.01193239e-03,  9.39836900e-04, ...,\n",
       "          5.62932692e-04,  3.27029382e-04, -7.34977657e-04],\n",
       "        [-7.28884363e-04,  1.46506273e-03,  6.82577898e-04, ...,\n",
       "          6.15564990e-04,  6.37201651e-04, -6.67457294e-04],\n",
       "        [-8.74429068e-04,  1.87660055e-03,  4.13665402e-04, ...,\n",
       "          6.60383026e-04,  9.46368556e-04, -5.40526176e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.32260684e-04,  1.76336165e-04,  2.23136449e-05, ...,\n",
       "          1.98939990e-04,  6.49734138e-05, -5.33336861e-05],\n",
       "        [-1.57056347e-04,  4.10561392e-04, -3.44399523e-05, ...,\n",
       "          1.32929044e-05,  3.47020163e-04,  2.57068450e-06],\n",
       "        [-3.91510810e-04,  3.93022172e-04, -9.19066879e-05, ...,\n",
       "          6.55498879e-05,  5.82576147e-04,  9.09129521e-05],\n",
       "        ...,\n",
       "        [-6.20870909e-04,  4.67221515e-04,  1.62860542e-03, ...,\n",
       "         -3.24716500e-04, -6.62351667e-04, -8.67152528e-04],\n",
       "        [-5.06941578e-04,  9.77206626e-04,  1.49289635e-03, ...,\n",
       "         -5.19282403e-05, -4.77631984e-04, -8.54503887e-04],\n",
       "        [-4.56534588e-04,  1.46509637e-03,  1.26672618e-03, ...,\n",
       "          1.87949074e-04, -2.19901922e-04, -8.00101028e-04]],\n",
       "\n",
       "       [[-1.32260684e-04,  1.76336165e-04,  2.23136449e-05, ...,\n",
       "          1.98939990e-04,  6.49734138e-05, -5.33336861e-05],\n",
       "        [ 1.27245105e-04,  3.60498874e-04, -1.40295364e-04, ...,\n",
       "         -1.17391774e-05,  1.29445369e-04, -5.63883805e-05],\n",
       "        [ 3.00357671e-04,  5.67407755e-04, -5.66164053e-05, ...,\n",
       "         -3.59113656e-05,  2.97251536e-05,  3.68577370e-04],\n",
       "        ...,\n",
       "        [-2.66639690e-04,  2.06070975e-03, -7.40118907e-04, ...,\n",
       "          4.81818017e-04,  4.62129683e-04, -4.29156789e-04],\n",
       "        [-4.05143393e-04,  2.31721718e-03, -8.40983528e-04, ...,\n",
       "          5.88276656e-04,  8.34792794e-04, -2.92943179e-04],\n",
       "        [-5.42390626e-04,  2.53425841e-03, -9.56355885e-04, ...,\n",
       "          6.80877944e-04,  1.17425097e-03, -1.23888196e-04]],\n",
       "\n",
       "       [[-1.32260684e-04,  1.76336165e-04,  2.23136449e-05, ...,\n",
       "          1.98939990e-04,  6.49734138e-05, -5.33336861e-05],\n",
       "        [-7.32161861e-05,  4.38729476e-04, -2.21946866e-05, ...,\n",
       "          4.22734447e-04,  1.77601483e-04,  3.16032783e-05],\n",
       "        [-7.98148722e-06,  3.90806439e-04,  3.26300215e-05, ...,\n",
       "          4.64964513e-04,  3.69253903e-05,  2.04813012e-04],\n",
       "        ...,\n",
       "        [-1.87990518e-04,  7.59711897e-04,  1.12203427e-03, ...,\n",
       "          1.12244999e-03,  7.23496545e-04,  2.38838635e-04],\n",
       "        [-2.52171303e-04,  1.18992466e-03,  1.08125398e-03, ...,\n",
       "          1.21560006e-03,  8.80267180e-04, -7.01483659e-05],\n",
       "        [-3.53532640e-04,  1.62564288e-03,  9.44595376e-04, ...,\n",
       "          1.26610673e-03,  1.07994874e-03, -2.77989369e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for enc_train, dec_train in dataset.take(1): break\n",
    "model(enc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bed31e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lyricist\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8569a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "610/610 [==============================] - 103s 165ms/step - loss: 3.4182\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 104s 170ms/step - loss: 2.9687\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 104s 170ms/step - loss: 2.7951\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 104s 170ms/step - loss: 2.6666\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 104s 170ms/step - loss: 2.5558\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 104s 170ms/step - loss: 2.4559\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 104s 171ms/step - loss: 2.3639\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 104s 171ms/step - loss: 2.2784\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 104s 171ms/step - loss: 2.1984\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 104s 171ms/step - loss: 2.1222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8496071040>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed0e9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b70f122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c4a55ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m not gonna be a poet <end> '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a85be2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i hate to see the <unk> <end> '"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i hate\", max_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5654c0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you love me , you know , you know <end> '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you love\", max_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "92c3057e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> don t look back , i m not gonna leave <end> '"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> don t look back\", max_len=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98623b",
   "metadata": {},
   "source": [
    "## 결과 분석\n",
    "\n",
    "1. 운좋게 파라미터값을 수정하지 않았는데도 기준치를 통과했다. 학습에 시간이 오래걸려서 다행이었다.\n",
    "\n",
    "2. 적절한 언어를 내놓는게 신기하긴 하지만 결과는 좀 아쉬운 편... 비슷한 단어만 계속 말하는 수준이다.\n",
    "\n",
    "3. nlp를 처음 접해봐서 조금 충격적이다. 글쓰는 것이 아무리 예술의 영역이라도 일정한 form이 있기 때문에\n",
    "  언젠간 작가란 직업이 사라질수도 있겠다는 생각이 든다. 인공지능의 가진 양면을 고민해보게 되었다.\n",
    "  \n",
    "4. 그치만 좀 이해가 안되는 부분은, 문법을 배제하는 점. 문법은 논리적인 부분이라 인간에게는 어려워도\n",
    "  컴퓨터에겐 오히려 쉬운 영역일텐데... 어째서 통계에만 의존하는지 잘모르겠다. \n",
    "  언어의 사용이라는 측면이 무에서 유를 창조하는거라곤 하지만, 가이드라인정도는 미리 줄 수 있을꺼도 같은데.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cdaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
